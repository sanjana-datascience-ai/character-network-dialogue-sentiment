{
  "best_global_step": 7830,
  "best_metric": 0.8745152256466449,
  "best_model_checkpoint": "C:\\Users\\sanja\\Documents\\4th year\\7sem\\Natural Language Processing\\Assignments\\Individual\\Implementation\\character-network-dialogue-sentiment\\models\\emotion_influence\\checkpoint-7830",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7830,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019157088122605363,
      "grad_norm": 6.185790061950684,
      "learning_rate": 1.9874840357598978e-05,
      "loss": 0.6368,
      "step": 50
    },
    {
      "epoch": 0.038314176245210725,
      "grad_norm": 2.1872971057891846,
      "learning_rate": 1.9747126436781613e-05,
      "loss": 0.6326,
      "step": 100
    },
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 3.361740827560425,
      "learning_rate": 1.961941251596424e-05,
      "loss": 0.6108,
      "step": 150
    },
    {
      "epoch": 0.07662835249042145,
      "grad_norm": 3.2700135707855225,
      "learning_rate": 1.9491698595146873e-05,
      "loss": 0.5987,
      "step": 200
    },
    {
      "epoch": 0.09578544061302682,
      "grad_norm": 2.7308566570281982,
      "learning_rate": 1.9363984674329505e-05,
      "loss": 0.5832,
      "step": 250
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 3.1751768589019775,
      "learning_rate": 1.9236270753512133e-05,
      "loss": 0.5315,
      "step": 300
    },
    {
      "epoch": 0.13409961685823754,
      "grad_norm": 1.9083665609359741,
      "learning_rate": 1.9108556832694765e-05,
      "loss": 0.6169,
      "step": 350
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 3.3846380710601807,
      "learning_rate": 1.8980842911877396e-05,
      "loss": 0.5674,
      "step": 400
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 1.761734962463379,
      "learning_rate": 1.8853128991060028e-05,
      "loss": 0.5856,
      "step": 450
    },
    {
      "epoch": 0.19157088122605365,
      "grad_norm": 3.225411891937256,
      "learning_rate": 1.872541507024266e-05,
      "loss": 0.5256,
      "step": 500
    },
    {
      "epoch": 0.210727969348659,
      "grad_norm": 3.200972318649292,
      "learning_rate": 1.8597701149425288e-05,
      "loss": 0.5398,
      "step": 550
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 3.8462719917297363,
      "learning_rate": 1.846998722860792e-05,
      "loss": 0.588,
      "step": 600
    },
    {
      "epoch": 0.24904214559386972,
      "grad_norm": 2.807803153991699,
      "learning_rate": 1.834227330779055e-05,
      "loss": 0.5604,
      "step": 650
    },
    {
      "epoch": 0.2681992337164751,
      "grad_norm": 1.246363639831543,
      "learning_rate": 1.821455938697318e-05,
      "loss": 0.5464,
      "step": 700
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 3.335827589035034,
      "learning_rate": 1.8086845466155815e-05,
      "loss": 0.5689,
      "step": 750
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 3.2773663997650146,
      "learning_rate": 1.7959131545338443e-05,
      "loss": 0.5742,
      "step": 800
    },
    {
      "epoch": 0.32567049808429116,
      "grad_norm": 1.9947830438613892,
      "learning_rate": 1.7831417624521075e-05,
      "loss": 0.5942,
      "step": 850
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 2.389636993408203,
      "learning_rate": 1.7703703703703706e-05,
      "loss": 0.5397,
      "step": 900
    },
    {
      "epoch": 0.36398467432950193,
      "grad_norm": 2.925682306289673,
      "learning_rate": 1.7575989782886335e-05,
      "loss": 0.5872,
      "step": 950
    },
    {
      "epoch": 0.3831417624521073,
      "grad_norm": 2.9193532466888428,
      "learning_rate": 1.7448275862068966e-05,
      "loss": 0.6475,
      "step": 1000
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 2.2713170051574707,
      "learning_rate": 1.7320561941251598e-05,
      "loss": 0.5754,
      "step": 1050
    },
    {
      "epoch": 0.421455938697318,
      "grad_norm": 2.50116229057312,
      "learning_rate": 1.719284802043423e-05,
      "loss": 0.5724,
      "step": 1100
    },
    {
      "epoch": 0.44061302681992337,
      "grad_norm": 2.901181221008301,
      "learning_rate": 1.706513409961686e-05,
      "loss": 0.5784,
      "step": 1150
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 2.6740896701812744,
      "learning_rate": 1.693742017879949e-05,
      "loss": 0.5183,
      "step": 1200
    },
    {
      "epoch": 0.4789272030651341,
      "grad_norm": 2.950580358505249,
      "learning_rate": 1.680970625798212e-05,
      "loss": 0.6529,
      "step": 1250
    },
    {
      "epoch": 0.49808429118773945,
      "grad_norm": 0.7425850629806519,
      "learning_rate": 1.6681992337164753e-05,
      "loss": 0.5479,
      "step": 1300
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 2.158095359802246,
      "learning_rate": 1.655427841634738e-05,
      "loss": 0.5281,
      "step": 1350
    },
    {
      "epoch": 0.5363984674329502,
      "grad_norm": 3.0014615058898926,
      "learning_rate": 1.6426564495530017e-05,
      "loss": 0.5678,
      "step": 1400
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.9919638633728027,
      "learning_rate": 1.6298850574712645e-05,
      "loss": 0.5432,
      "step": 1450
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 2.8159842491149902,
      "learning_rate": 1.6171136653895276e-05,
      "loss": 0.5827,
      "step": 1500
    },
    {
      "epoch": 0.5938697318007663,
      "grad_norm": 1.371791958808899,
      "learning_rate": 1.6043422733077908e-05,
      "loss": 0.5604,
      "step": 1550
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 1.4784098863601685,
      "learning_rate": 1.5915708812260536e-05,
      "loss": 0.5375,
      "step": 1600
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 2.0918655395507812,
      "learning_rate": 1.5787994891443168e-05,
      "loss": 0.4759,
      "step": 1650
    },
    {
      "epoch": 0.6513409961685823,
      "grad_norm": 2.8079051971435547,
      "learning_rate": 1.56602809706258e-05,
      "loss": 0.6042,
      "step": 1700
    },
    {
      "epoch": 0.6704980842911877,
      "grad_norm": 2.3848650455474854,
      "learning_rate": 1.553256704980843e-05,
      "loss": 0.6339,
      "step": 1750
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 2.7715206146240234,
      "learning_rate": 1.5404853128991063e-05,
      "loss": 0.5293,
      "step": 1800
    },
    {
      "epoch": 0.7088122605363985,
      "grad_norm": 1.413261890411377,
      "learning_rate": 1.527713920817369e-05,
      "loss": 0.5615,
      "step": 1850
    },
    {
      "epoch": 0.7279693486590039,
      "grad_norm": 1.8770747184753418,
      "learning_rate": 1.5149425287356323e-05,
      "loss": 0.5149,
      "step": 1900
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 4.028753757476807,
      "learning_rate": 1.5021711366538953e-05,
      "loss": 0.5199,
      "step": 1950
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 2.298642635345459,
      "learning_rate": 1.4893997445721585e-05,
      "loss": 0.6533,
      "step": 2000
    },
    {
      "epoch": 0.7854406130268199,
      "grad_norm": 2.025470018386841,
      "learning_rate": 1.4766283524904215e-05,
      "loss": 0.5442,
      "step": 2050
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 1.788602590560913,
      "learning_rate": 1.4638569604086848e-05,
      "loss": 0.5941,
      "step": 2100
    },
    {
      "epoch": 0.8237547892720306,
      "grad_norm": 1.438094139099121,
      "learning_rate": 1.4510855683269478e-05,
      "loss": 0.4739,
      "step": 2150
    },
    {
      "epoch": 0.842911877394636,
      "grad_norm": 3.3309385776519775,
      "learning_rate": 1.4383141762452108e-05,
      "loss": 0.497,
      "step": 2200
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 0.8614579439163208,
      "learning_rate": 1.425542784163474e-05,
      "loss": 0.4978,
      "step": 2250
    },
    {
      "epoch": 0.8812260536398467,
      "grad_norm": 2.2102530002593994,
      "learning_rate": 1.412771392081737e-05,
      "loss": 0.5314,
      "step": 2300
    },
    {
      "epoch": 0.9003831417624522,
      "grad_norm": 2.3401169776916504,
      "learning_rate": 1.4e-05,
      "loss": 0.5057,
      "step": 2350
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 1.34648859500885,
      "learning_rate": 1.3872286079182633e-05,
      "loss": 0.6105,
      "step": 2400
    },
    {
      "epoch": 0.9386973180076629,
      "grad_norm": 2.1395835876464844,
      "learning_rate": 1.3744572158365263e-05,
      "loss": 0.5697,
      "step": 2450
    },
    {
      "epoch": 0.9578544061302682,
      "grad_norm": 1.635932445526123,
      "learning_rate": 1.3616858237547895e-05,
      "loss": 0.6067,
      "step": 2500
    },
    {
      "epoch": 0.9770114942528736,
      "grad_norm": 1.5112230777740479,
      "learning_rate": 1.3489144316730525e-05,
      "loss": 0.566,
      "step": 2550
    },
    {
      "epoch": 0.9961685823754789,
      "grad_norm": 2.5618948936462402,
      "learning_rate": 1.3361430395913155e-05,
      "loss": 0.5046,
      "step": 2600
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8733231123035646,
      "eval_f1_macro": 0.8732069409098209,
      "eval_loss": 0.5496853590011597,
      "eval_runtime": 225.0298,
      "eval_samples_per_second": 23.188,
      "eval_steps_per_second": 1.453,
      "step": 2610
    },
    {
      "epoch": 1.0153256704980842,
      "grad_norm": 2.6172945499420166,
      "learning_rate": 1.3233716475095786e-05,
      "loss": 0.5532,
      "step": 2650
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 1.5034579038619995,
      "learning_rate": 1.3106002554278416e-05,
      "loss": 0.5762,
      "step": 2700
    },
    {
      "epoch": 1.053639846743295,
      "grad_norm": 4.326416492462158,
      "learning_rate": 1.297828863346105e-05,
      "loss": 0.5209,
      "step": 2750
    },
    {
      "epoch": 1.0727969348659003,
      "grad_norm": 2.102548360824585,
      "learning_rate": 1.285057471264368e-05,
      "loss": 0.5322,
      "step": 2800
    },
    {
      "epoch": 1.0919540229885056,
      "grad_norm": 2.0055298805236816,
      "learning_rate": 1.272286079182631e-05,
      "loss": 0.6129,
      "step": 2850
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.5888062715530396,
      "learning_rate": 1.2595146871008941e-05,
      "loss": 0.6141,
      "step": 2900
    },
    {
      "epoch": 1.1302681992337165,
      "grad_norm": 1.9632363319396973,
      "learning_rate": 1.2467432950191571e-05,
      "loss": 0.5865,
      "step": 2950
    },
    {
      "epoch": 1.1494252873563218,
      "grad_norm": 2.1012070178985596,
      "learning_rate": 1.2339719029374201e-05,
      "loss": 0.5806,
      "step": 3000
    },
    {
      "epoch": 1.1685823754789273,
      "grad_norm": 1.101115107536316,
      "learning_rate": 1.2212005108556835e-05,
      "loss": 0.586,
      "step": 3050
    },
    {
      "epoch": 1.1877394636015326,
      "grad_norm": 1.558437466621399,
      "learning_rate": 1.2084291187739465e-05,
      "loss": 0.5229,
      "step": 3100
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 2.353973150253296,
      "learning_rate": 1.1956577266922096e-05,
      "loss": 0.5354,
      "step": 3150
    },
    {
      "epoch": 1.2260536398467432,
      "grad_norm": 2.05458664894104,
      "learning_rate": 1.1828863346104726e-05,
      "loss": 0.5938,
      "step": 3200
    },
    {
      "epoch": 1.2452107279693487,
      "grad_norm": 2.5776209831237793,
      "learning_rate": 1.1701149425287356e-05,
      "loss": 0.5684,
      "step": 3250
    },
    {
      "epoch": 1.264367816091954,
      "grad_norm": 1.0841764211654663,
      "learning_rate": 1.1573435504469988e-05,
      "loss": 0.5115,
      "step": 3300
    },
    {
      "epoch": 1.2835249042145593,
      "grad_norm": 3.405813694000244,
      "learning_rate": 1.1445721583652618e-05,
      "loss": 0.6053,
      "step": 3350
    },
    {
      "epoch": 1.3026819923371646,
      "grad_norm": 3.1781768798828125,
      "learning_rate": 1.1318007662835252e-05,
      "loss": 0.5403,
      "step": 3400
    },
    {
      "epoch": 1.3218390804597702,
      "grad_norm": 0.6297923922538757,
      "learning_rate": 1.1190293742017881e-05,
      "loss": 0.5082,
      "step": 3450
    },
    {
      "epoch": 1.3409961685823755,
      "grad_norm": 1.7784829139709473,
      "learning_rate": 1.1062579821200511e-05,
      "loss": 0.6031,
      "step": 3500
    },
    {
      "epoch": 1.3601532567049808,
      "grad_norm": 2.3400115966796875,
      "learning_rate": 1.0934865900383143e-05,
      "loss": 0.5052,
      "step": 3550
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 1.3448288440704346,
      "learning_rate": 1.0807151979565773e-05,
      "loss": 0.5225,
      "step": 3600
    },
    {
      "epoch": 1.3984674329501916,
      "grad_norm": 4.220954895019531,
      "learning_rate": 1.0679438058748403e-05,
      "loss": 0.5872,
      "step": 3650
    },
    {
      "epoch": 1.417624521072797,
      "grad_norm": 0.6174890995025635,
      "learning_rate": 1.0551724137931037e-05,
      "loss": 0.5772,
      "step": 3700
    },
    {
      "epoch": 1.4367816091954024,
      "grad_norm": 2.6943602561950684,
      "learning_rate": 1.0424010217113666e-05,
      "loss": 0.5576,
      "step": 3750
    },
    {
      "epoch": 1.4559386973180077,
      "grad_norm": 2.1912930011749268,
      "learning_rate": 1.0296296296296298e-05,
      "loss": 0.5014,
      "step": 3800
    },
    {
      "epoch": 1.475095785440613,
      "grad_norm": 1.7535316944122314,
      "learning_rate": 1.0168582375478928e-05,
      "loss": 0.5151,
      "step": 3850
    },
    {
      "epoch": 1.4942528735632183,
      "grad_norm": 1.0006999969482422,
      "learning_rate": 1.0040868454661558e-05,
      "loss": 0.5849,
      "step": 3900
    },
    {
      "epoch": 1.5134099616858236,
      "grad_norm": 1.765085220336914,
      "learning_rate": 9.91315453384419e-06,
      "loss": 0.432,
      "step": 3950
    },
    {
      "epoch": 1.5325670498084292,
      "grad_norm": 1.4744917154312134,
      "learning_rate": 9.785440613026821e-06,
      "loss": 0.5517,
      "step": 4000
    },
    {
      "epoch": 1.5517241379310345,
      "grad_norm": 4.6141557693481445,
      "learning_rate": 9.657726692209451e-06,
      "loss": 0.5724,
      "step": 4050
    },
    {
      "epoch": 1.5708812260536398,
      "grad_norm": 2.4225857257843018,
      "learning_rate": 9.530012771392081e-06,
      "loss": 0.4655,
      "step": 4100
    },
    {
      "epoch": 1.5900383141762453,
      "grad_norm": 2.0800106525421143,
      "learning_rate": 9.402298850574713e-06,
      "loss": 0.5485,
      "step": 4150
    },
    {
      "epoch": 1.6091954022988506,
      "grad_norm": 2.105010509490967,
      "learning_rate": 9.274584929757345e-06,
      "loss": 0.5228,
      "step": 4200
    },
    {
      "epoch": 1.628352490421456,
      "grad_norm": 1.9462887048721313,
      "learning_rate": 9.146871008939975e-06,
      "loss": 0.5282,
      "step": 4250
    },
    {
      "epoch": 1.6475095785440614,
      "grad_norm": 2.304734706878662,
      "learning_rate": 9.019157088122606e-06,
      "loss": 0.4702,
      "step": 4300
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.6927887201309204,
      "learning_rate": 8.891443167305236e-06,
      "loss": 0.5115,
      "step": 4350
    },
    {
      "epoch": 1.685823754789272,
      "grad_norm": 1.0741537809371948,
      "learning_rate": 8.763729246487868e-06,
      "loss": 0.5903,
      "step": 4400
    },
    {
      "epoch": 1.7049808429118773,
      "grad_norm": 7.131383895874023,
      "learning_rate": 8.6360153256705e-06,
      "loss": 0.5435,
      "step": 4450
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 3.038825750350952,
      "learning_rate": 8.50830140485313e-06,
      "loss": 0.5411,
      "step": 4500
    },
    {
      "epoch": 1.7432950191570882,
      "grad_norm": 2.088927745819092,
      "learning_rate": 8.38058748403576e-06,
      "loss": 0.585,
      "step": 4550
    },
    {
      "epoch": 1.7624521072796935,
      "grad_norm": 1.7922850847244263,
      "learning_rate": 8.252873563218391e-06,
      "loss": 0.5295,
      "step": 4600
    },
    {
      "epoch": 1.7816091954022988,
      "grad_norm": 1.8914316892623901,
      "learning_rate": 8.125159642401023e-06,
      "loss": 0.5651,
      "step": 4650
    },
    {
      "epoch": 1.8007662835249043,
      "grad_norm": 1.8898860216140747,
      "learning_rate": 7.997445721583653e-06,
      "loss": 0.5412,
      "step": 4700
    },
    {
      "epoch": 1.8199233716475096,
      "grad_norm": 2.1605324745178223,
      "learning_rate": 7.869731800766283e-06,
      "loss": 0.5556,
      "step": 4750
    },
    {
      "epoch": 1.839080459770115,
      "grad_norm": 2.725102663040161,
      "learning_rate": 7.742017879948915e-06,
      "loss": 0.5216,
      "step": 4800
    },
    {
      "epoch": 1.8582375478927204,
      "grad_norm": 4.20106840133667,
      "learning_rate": 7.614303959131546e-06,
      "loss": 0.5838,
      "step": 4850
    },
    {
      "epoch": 1.8773946360153255,
      "grad_norm": 1.14970862865448,
      "learning_rate": 7.4865900383141765e-06,
      "loss": 0.5659,
      "step": 4900
    },
    {
      "epoch": 1.896551724137931,
      "grad_norm": 2.5271379947662354,
      "learning_rate": 7.358876117496808e-06,
      "loss": 0.5353,
      "step": 4950
    },
    {
      "epoch": 1.9157088122605364,
      "grad_norm": 3.457995653152466,
      "learning_rate": 7.231162196679439e-06,
      "loss": 0.581,
      "step": 5000
    },
    {
      "epoch": 1.9348659003831417,
      "grad_norm": 2.939809799194336,
      "learning_rate": 7.103448275862069e-06,
      "loss": 0.5388,
      "step": 5050
    },
    {
      "epoch": 1.9540229885057472,
      "grad_norm": 2.3419487476348877,
      "learning_rate": 6.975734355044701e-06,
      "loss": 0.5548,
      "step": 5100
    },
    {
      "epoch": 1.9731800766283525,
      "grad_norm": 7.293647289276123,
      "learning_rate": 6.8480204342273315e-06,
      "loss": 0.4938,
      "step": 5150
    },
    {
      "epoch": 1.9923371647509578,
      "grad_norm": 2.412381172180176,
      "learning_rate": 6.720306513409962e-06,
      "loss": 0.6144,
      "step": 5200
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8740896895362208,
      "eval_f1_macro": 0.8739464566203337,
      "eval_loss": 0.5288419723510742,
      "eval_runtime": 225.593,
      "eval_samples_per_second": 23.13,
      "eval_steps_per_second": 1.45,
      "step": 5220
    },
    {
      "epoch": 2.0114942528735633,
      "grad_norm": 4.093355655670166,
      "learning_rate": 6.592592592592592e-06,
      "loss": 0.5178,
      "step": 5250
    },
    {
      "epoch": 2.0306513409961684,
      "grad_norm": 3.1556077003479004,
      "learning_rate": 6.464878671775224e-06,
      "loss": 0.5483,
      "step": 5300
    },
    {
      "epoch": 2.049808429118774,
      "grad_norm": 0.6974784731864929,
      "learning_rate": 6.337164750957855e-06,
      "loss": 0.4225,
      "step": 5350
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 2.3178679943084717,
      "learning_rate": 6.209450830140486e-06,
      "loss": 0.5911,
      "step": 5400
    },
    {
      "epoch": 2.0881226053639845,
      "grad_norm": 6.746016502380371,
      "learning_rate": 6.081736909323117e-06,
      "loss": 0.5404,
      "step": 5450
    },
    {
      "epoch": 2.10727969348659,
      "grad_norm": 6.60444974899292,
      "learning_rate": 5.954022988505747e-06,
      "loss": 0.5155,
      "step": 5500
    },
    {
      "epoch": 2.1264367816091956,
      "grad_norm": 3.9206104278564453,
      "learning_rate": 5.826309067688378e-06,
      "loss": 0.564,
      "step": 5550
    },
    {
      "epoch": 2.1455938697318007,
      "grad_norm": 3.9575936794281006,
      "learning_rate": 5.69859514687101e-06,
      "loss": 0.5451,
      "step": 5600
    },
    {
      "epoch": 2.164750957854406,
      "grad_norm": 2.1949405670166016,
      "learning_rate": 5.570881226053641e-06,
      "loss": 0.5244,
      "step": 5650
    },
    {
      "epoch": 2.1839080459770113,
      "grad_norm": 4.954605579376221,
      "learning_rate": 5.443167305236271e-06,
      "loss": 0.5568,
      "step": 5700
    },
    {
      "epoch": 2.203065134099617,
      "grad_norm": 5.148554801940918,
      "learning_rate": 5.3154533844189015e-06,
      "loss": 0.5404,
      "step": 5750
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 6.507801532745361,
      "learning_rate": 5.187739463601533e-06,
      "loss": 0.5859,
      "step": 5800
    },
    {
      "epoch": 2.2413793103448274,
      "grad_norm": 2.6840527057647705,
      "learning_rate": 5.060025542784164e-06,
      "loss": 0.4787,
      "step": 5850
    },
    {
      "epoch": 2.260536398467433,
      "grad_norm": 3.530020236968994,
      "learning_rate": 4.932311621966795e-06,
      "loss": 0.5829,
      "step": 5900
    },
    {
      "epoch": 2.2796934865900385,
      "grad_norm": 2.7716948986053467,
      "learning_rate": 4.804597701149426e-06,
      "loss": 0.5965,
      "step": 5950
    },
    {
      "epoch": 2.2988505747126435,
      "grad_norm": 1.1417635679244995,
      "learning_rate": 4.6768837803320565e-06,
      "loss": 0.4797,
      "step": 6000
    },
    {
      "epoch": 2.318007662835249,
      "grad_norm": 1.6224303245544434,
      "learning_rate": 4.549169859514687e-06,
      "loss": 0.5784,
      "step": 6050
    },
    {
      "epoch": 2.3371647509578546,
      "grad_norm": 1.7599914073944092,
      "learning_rate": 4.421455938697318e-06,
      "loss": 0.5685,
      "step": 6100
    },
    {
      "epoch": 2.3563218390804597,
      "grad_norm": 1.1365416049957275,
      "learning_rate": 4.293742017879949e-06,
      "loss": 0.5553,
      "step": 6150
    },
    {
      "epoch": 2.375478927203065,
      "grad_norm": 9.557280540466309,
      "learning_rate": 4.16602809706258e-06,
      "loss": 0.5563,
      "step": 6200
    },
    {
      "epoch": 2.3946360153256707,
      "grad_norm": 4.834929466247559,
      "learning_rate": 4.038314176245211e-06,
      "loss": 0.4817,
      "step": 6250
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 3.902817726135254,
      "learning_rate": 3.910600255427842e-06,
      "loss": 0.4798,
      "step": 6300
    },
    {
      "epoch": 2.4329501915708813,
      "grad_norm": 5.494172096252441,
      "learning_rate": 3.7828863346104727e-06,
      "loss": 0.5014,
      "step": 6350
    },
    {
      "epoch": 2.4521072796934864,
      "grad_norm": 7.729676723480225,
      "learning_rate": 3.655172413793104e-06,
      "loss": 0.5413,
      "step": 6400
    },
    {
      "epoch": 2.471264367816092,
      "grad_norm": 0.7529755234718323,
      "learning_rate": 3.5274584929757344e-06,
      "loss": 0.6034,
      "step": 6450
    },
    {
      "epoch": 2.4904214559386975,
      "grad_norm": 3.655702829360962,
      "learning_rate": 3.3997445721583657e-06,
      "loss": 0.528,
      "step": 6500
    },
    {
      "epoch": 2.5095785440613025,
      "grad_norm": 14.828399658203125,
      "learning_rate": 3.2720306513409965e-06,
      "loss": 0.5555,
      "step": 6550
    },
    {
      "epoch": 2.528735632183908,
      "grad_norm": 0.702721118927002,
      "learning_rate": 3.1443167305236273e-06,
      "loss": 0.533,
      "step": 6600
    },
    {
      "epoch": 2.547892720306513,
      "grad_norm": 2.857717514038086,
      "learning_rate": 3.016602809706258e-06,
      "loss": 0.5448,
      "step": 6650
    },
    {
      "epoch": 2.5670498084291187,
      "grad_norm": 0.9410836696624756,
      "learning_rate": 2.888888888888889e-06,
      "loss": 0.5378,
      "step": 6700
    },
    {
      "epoch": 2.586206896551724,
      "grad_norm": 5.364953994750977,
      "learning_rate": 2.76117496807152e-06,
      "loss": 0.4679,
      "step": 6750
    },
    {
      "epoch": 2.6053639846743293,
      "grad_norm": 2.7665598392486572,
      "learning_rate": 2.633461047254151e-06,
      "loss": 0.5454,
      "step": 6800
    },
    {
      "epoch": 2.624521072796935,
      "grad_norm": 3.1414566040039062,
      "learning_rate": 2.5057471264367815e-06,
      "loss": 0.4615,
      "step": 6850
    },
    {
      "epoch": 2.6436781609195403,
      "grad_norm": 2.245986223220825,
      "learning_rate": 2.3780332056194127e-06,
      "loss": 0.5353,
      "step": 6900
    },
    {
      "epoch": 2.6628352490421454,
      "grad_norm": 10.26122760772705,
      "learning_rate": 2.2503192848020436e-06,
      "loss": 0.5063,
      "step": 6950
    },
    {
      "epoch": 2.681992337164751,
      "grad_norm": 1.8588193655014038,
      "learning_rate": 2.1226053639846744e-06,
      "loss": 0.5225,
      "step": 7000
    },
    {
      "epoch": 2.7011494252873565,
      "grad_norm": 5.967404365539551,
      "learning_rate": 1.9948914431673052e-06,
      "loss": 0.5058,
      "step": 7050
    },
    {
      "epoch": 2.7203065134099615,
      "grad_norm": 2.529646873474121,
      "learning_rate": 1.8671775223499363e-06,
      "loss": 0.5034,
      "step": 7100
    },
    {
      "epoch": 2.739463601532567,
      "grad_norm": 9.175411224365234,
      "learning_rate": 1.7394636015325671e-06,
      "loss": 0.5194,
      "step": 7150
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 4.691681861877441,
      "learning_rate": 1.611749680715198e-06,
      "loss": 0.4757,
      "step": 7200
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 2.112724781036377,
      "learning_rate": 1.4840357598978288e-06,
      "loss": 0.4859,
      "step": 7250
    },
    {
      "epoch": 2.796934865900383,
      "grad_norm": 6.1082329750061035,
      "learning_rate": 1.35632183908046e-06,
      "loss": 0.535,
      "step": 7300
    },
    {
      "epoch": 2.8160919540229887,
      "grad_norm": 1.8166640996932983,
      "learning_rate": 1.2286079182630909e-06,
      "loss": 0.4927,
      "step": 7350
    },
    {
      "epoch": 2.835249042145594,
      "grad_norm": 3.3786935806274414,
      "learning_rate": 1.1008939974457217e-06,
      "loss": 0.4965,
      "step": 7400
    },
    {
      "epoch": 2.8544061302681993,
      "grad_norm": 4.318451404571533,
      "learning_rate": 9.731800766283525e-07,
      "loss": 0.5109,
      "step": 7450
    },
    {
      "epoch": 2.873563218390805,
      "grad_norm": 7.862526893615723,
      "learning_rate": 8.454661558109835e-07,
      "loss": 0.4724,
      "step": 7500
    },
    {
      "epoch": 2.89272030651341,
      "grad_norm": 0.8592365980148315,
      "learning_rate": 7.177522349936143e-07,
      "loss": 0.5169,
      "step": 7550
    },
    {
      "epoch": 2.9118773946360155,
      "grad_norm": 5.852635860443115,
      "learning_rate": 5.900383141762452e-07,
      "loss": 0.4618,
      "step": 7600
    },
    {
      "epoch": 2.9310344827586206,
      "grad_norm": 4.163909912109375,
      "learning_rate": 4.6232439335887613e-07,
      "loss": 0.5643,
      "step": 7650
    },
    {
      "epoch": 2.950191570881226,
      "grad_norm": 2.826422929763794,
      "learning_rate": 3.34610472541507e-07,
      "loss": 0.6037,
      "step": 7700
    },
    {
      "epoch": 2.969348659003831,
      "grad_norm": 3.4255335330963135,
      "learning_rate": 2.0689655172413796e-07,
      "loss": 0.5016,
      "step": 7750
    },
    {
      "epoch": 2.9885057471264367,
      "grad_norm": 5.798701286315918,
      "learning_rate": 7.918263090676886e-08,
      "loss": 0.5379,
      "step": 7800
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.875047911077041,
      "eval_f1_macro": 0.8745152256466449,
      "eval_loss": 0.5203845500946045,
      "eval_runtime": 225.5663,
      "eval_samples_per_second": 23.133,
      "eval_steps_per_second": 1.45,
      "step": 7830
    }
  ],
  "logging_steps": 50,
  "max_steps": 7830,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4147758481616640.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
